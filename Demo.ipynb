{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "binding-glance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "046347d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\polin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\polin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "informal-powell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter nbextension enable --py widgetsnbextension --sys-prefix\n",
    "# !jupyter serverextension enable voila --sys-prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57f3ca97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_news_from_main_web_page():\n",
    "    news_data = {'Article title': [], 'Link': [], 'text': [], 'cleaned_text': [], 'Date': [], 'Categories': [], 'Predicted categories': []}\n",
    "    page = \"http://catdailynews.com/\"\n",
    "    protocol_list = requests.get(page).text\n",
    "    soup = BeautifulSoup(protocol_list, 'lxml')\n",
    "    \n",
    "    for ai in soup.find_all('a'):\n",
    "        if ai.div:\n",
    "            news_data['Article title'].append(ai.div.text)\n",
    "            news_data['Link'].append(ai.get('href'))\n",
    "    return news_data\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([w if w not in eng_stopwords else ' ' for w in text.split(' ')])\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return \"\".join([ch if ch not in string.punctuation else ' ' for ch in text])\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return ''.join([i if not i.isdigit() else ' ' for i in text])\n",
    "\n",
    "def remove_multiple_spaces(text):\n",
    "    return re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return \" \".join([lemma.lemmatize(word.lower()) for word in text.split(' ')])\n",
    "\n",
    "def news_text_cleaning(text):\n",
    "    text_removed_stopwords = remove_stopwords(text)\n",
    "    text_removed_punctuation = remove_punctuation(text_removed_stopwords)\n",
    "    text_removed_numbers = remove_numbers(text_removed_punctuation)\n",
    "    text_removed_multiple_spaces = remove_multiple_spaces(text_removed_numbers)\n",
    "    text_cleaned = lemmatize_text(text_removed_multiple_spaces)\n",
    "    return text_cleaned\n",
    "\n",
    "def predict_humor(text):\n",
    "    return humor_clf.predict([text])\n",
    "\n",
    "def predict_other_categories(text):\n",
    "    return other_categ_clf.predict([text])\n",
    "\n",
    "def predict_categories(text):\n",
    "    predicted_humor = predict_humor(text)\n",
    "    predicted_humor = str(predicted_humor[0])\n",
    "    predicted_other_categ = predict_other_categories(text)\n",
    "    predicted_other_categ = str(predicted_other_categ[0])\n",
    "    if predicted_humor == 'not Humor':\n",
    "        return predicted_other_categ\n",
    "    elif predicted_humor == 'Humor' and predicted_other_categ == 'No category':\n",
    "        return predicted_humor\n",
    "    else:\n",
    "        return predicted_humor + ', ' + predicted_other_categ\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "eng_stopwords = stopwords.words(\"english\")\n",
    "eng_stopwords.extend(['…', '«', '»', '...'])\n",
    "\n",
    "with open(\"humor_clf.pkl\", \"rb\") as file:\n",
    "    humor_clf = pickle.load(file)\n",
    "    \n",
    "with open(\"other_categ_clf.pkl\", \"rb\") as file:\n",
    "    other_categ_clf = pickle.load(file)\n",
    "\n",
    "def predict_category(news_data):\n",
    "    for idx, link in enumerate(news_data['Link']):\n",
    "        protocol_list = requests.get(link).text\n",
    "        soup = BeautifulSoup(protocol_list, 'lxml')\n",
    "\n",
    "        text_parts = soup.find_all('p')\n",
    "        text = ''\n",
    "        for p in text_parts:\n",
    "            text += p.text\n",
    "        news_data['text'].append(text)\n",
    "        text_cleaned = news_text_cleaning(text)\n",
    "        news_data['cleaned_text'].append(text_cleaned)\n",
    "\n",
    "        for divi in soup.find_all('div'):\n",
    "            if divi.get(\"class\") == ['pull-left']:\n",
    "                date = divi.text.split('  ')[0][:-2]\n",
    "        news_data['Date'].append(date)\n",
    "\n",
    "        categories = []\n",
    "        for divi in soup.find_all('div'):\n",
    "            if divi.get(\"class\") == ['category-tag']:\n",
    "                all_a = divi.find_all('a')\n",
    "                for a in all_a:\n",
    "                    if a.text[0].isupper():\n",
    "                        categories.append(a.text)\n",
    "                categories = ', '.join(categories)\n",
    "        news_data['Categories'].append(categories)\n",
    "\n",
    "        pred_categories = predict_categories(text_cleaned)\n",
    "        news_data['Predicted categories'].append(pred_categories)\n",
    "    return news_data\n",
    "\n",
    "def make_clickable(val):\n",
    "    # target _blank to open new window\n",
    "    return '<a target=\"_blank\" href=\"{}\">{}</a>'.format(val, val)\n",
    "\n",
    "def create_news_table_show(news_data):\n",
    "    news_data_table = pd.DataFrame(news_data)\n",
    "    news_table_show = news_data_table[['Date', 'Article title', 'Link', 'Categories', 'Predicted categories']]\n",
    "    news_table_show = news_table_show.style.format({'Link': make_clickable})\n",
    "    return news_table_show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "reflected-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Widget\n",
    "\n",
    "file = open(\"cat_daily_news_main.jpg\", \"rb\")\n",
    "main_image = file.read()\n",
    "\n",
    "file = open(\"sleepy_kitten.jpg\", \"rb\")\n",
    "lovely_image = file.read()\n",
    "\n",
    "image_headline = widgets.Image(\n",
    "                    value=main_image,\n",
    "                    format='jpg',\n",
    "                    width='95%',\n",
    "                    height='500px'\n",
    "                )\n",
    "\n",
    "image_base = widgets.Image(\n",
    "                    value=lovely_image,\n",
    "                    format='jpg',\n",
    "                    width='350px',\n",
    "                )\n",
    "\n",
    "label_base = widgets.Label(\n",
    "                    value='Photo by fanibani',\n",
    "                    style={'description_width': 'initial'}\n",
    "                )\n",
    "\n",
    "\n",
    "vbox_headline_image = widgets.VBox([image_headline])\n",
    "vbox_base_image = widgets.VBox([image_base, label_base])\n",
    "# display(vbox_headline_image, vbox_base_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa0fef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# button to load news\n",
    "\n",
    "button_send = widgets.Button(\n",
    "                description='Load news',\n",
    "                tooltip='Send',\n",
    "                style={'description_width': 'initial'}\n",
    "            )\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_button_clicked(event):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        news_data = read_news_from_main_web_page()\n",
    "        news_data_predicted = predict_category(news_data)\n",
    "        news_table_show = create_news_table_show(news_data_predicted)\n",
    "        display(news_table_show)\n",
    "        \n",
    "button_send.on_click(on_button_clicked)\n",
    "\n",
    "vbox_result = widgets.VBox([button_send, output])\n",
    "# display(vbox_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d628ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# news table widget\n",
    "\n",
    "# def news_table_widget_creation(df):\n",
    "    \n",
    "#     out = widgets.Output(layout={'border': '1px solid black'})\n",
    "#     with out:\n",
    "#         display(df)\n",
    "#     return out\n",
    "\n",
    "# news_table = news_table_widget_creation(news_table_show)\n",
    "# vbox_news_table = widgets.VBox([news_table])\n",
    "# display(vbox_news_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "660e3a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84e9c3f9f22474ab1e71cb9a770abcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00\\xf0\\x00\\xf0\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c61b6f4052ee44be8b7b15a309670369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe1\"\\x98Exif\\x00\\x00II*\\x00\\x08\\x00\\x00\\x00\\x13\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "level1 = widgets.HBox([vbox_headline_image])\n",
    "level2 = widgets.HBox([vbox_base_image, vbox_result])\n",
    "# level2 = widgets.HBox([vbox_result])\n",
    "# level2 = widgets.HBox([vbox_images, vbox_result])\n",
    "display(level1,level2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "according-supervisor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda list -e > requirements.txt\n",
    "# !pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dab667",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
