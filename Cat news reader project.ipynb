{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fc1975c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **Data Science Workplow**\n",
    "\n",
    "Berliner Hochschule für Technik\n",
    "\n",
    "News Reader project - Cat news\n",
    "\n",
    "Polina Kozyr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa8c72c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this project I extracted news about cats from cat news webpage http://catdailynews.com/ and made topic classification and sentiment analysis of these news. For topic classification I trained classificator to predict topic of cat news. For sentiment analysis I just used already pre-trained model to classify news on positive and negative ones. Also I made a web-application to make an end-to-end solution for the cat news reader.\n",
    "\n",
    "Project content:\n",
    "\n",
    "1. Extraction of news from website - creation of cat news dataset\n",
    "2. Topic classification\n",
    "        1) Cleaning of news texts\n",
    "        2) Formation of topic classification system stages\n",
    "        2) Classification for humorous or not humorous articles\n",
    "        3) Classification for other categories\n",
    "        4) Сombining results from two classifications\n",
    "3. Sentiment analysis\n",
    "4. Web-application creation\n",
    "\n",
    "I also added code that I used to create a web-application in the end of this NoteBook. Web-application extract recent news from http://catdailynews.com/, proccess them, predict their categories and sentiment and print results. \n",
    "\n",
    "Link for web-application: https://mybinder.org/v2/gh/polinatrump/news_reader_webapp_demo/HEAD?urlpath=%2Fvoila%2Frender%2FDemo.ipynb\n",
    "\n",
    "Web-app loading can take some time, sometimes really long."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06c78d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Extraction of news from website - creation of cat news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3954d338",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92240f8c",
   "metadata": {},
   "source": [
    "### Get title and link of news from pages that have list of news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "92aa8bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = []\n",
    "for subpage_index in range(1, 200):\n",
    "    page = \"http://catdailynews.com/page/\" + str(subpage_index) + \"/\"\n",
    "#     print(page)\n",
    "    protocol_list = requests.get(page).text\n",
    "    soup = BeautifulSoup(protocol_list, 'lxml')\n",
    "    \n",
    "    for ai in soup.find_all('a'):\n",
    "        if ai.div:\n",
    "            title = ai.div.text\n",
    "            link = ai.get('href')\n",
    "#             print(title)\n",
    "#             print(link)\n",
    "            news_data.append((title, link))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a3cd3644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990 news links were extracted from catnews webpages.\n"
     ]
    }
   ],
   "source": [
    "print(str(len(news_data)) + ' news links were extracted from catnews webpages.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b0ba6d",
   "metadata": {},
   "source": [
    "### Get  text, category and date from each news link page and write news data to the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2f0815a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "\n",
    "for idx, i in enumerate(news_data):\n",
    "#     n +=1\n",
    "#     print(n)\n",
    "    \n",
    "    protocol_list = requests.get(i[1]).text\n",
    "    soup = BeautifulSoup(protocol_list, 'lxml')\n",
    "    \n",
    "    text_parts = soup.find_all('p')\n",
    "    text = ''\n",
    "    for p in text_parts:\n",
    "        text += p.text\n",
    "    \n",
    "    for divi in soup.find_all('div'):\n",
    "        if divi.get(\"class\") == ['pull-left']:\n",
    "            date = divi.text.split('  ')[0][:-2]\n",
    "                \n",
    "    categories = []\n",
    "    for divi in soup.find_all('div'):\n",
    "        if divi.get(\"class\") == ['category-tag']:\n",
    "            all_a = divi.find_all('a')\n",
    "            for a in all_a:\n",
    "                if a.text[0].isupper():\n",
    "                    categories.append(a.text)\n",
    "            categories = ', '.join(categories)\n",
    "            \n",
    "    with open('news200.csv', 'a', newline='', encoding=\"utf-8\") as csvfile:\n",
    "        spamwriter = csv.writer(csvfile, delimiter='|')\n",
    "        spamwriter.writerow([i[0], i[1], date, text, categories])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e1ff30",
   "metadata": {},
   "source": [
    "## 2. Topic classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00072743",
   "metadata": {},
   "source": [
    "### Import cat news data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "74e8370c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Indiana Jones and a Cat</td>\n",
       "      <td>http://catdailynews.com/2023/06/indiana-jones-...</td>\n",
       "      <td>June 30, 2023</td>\n",
       "      <td>With the latest Indiana Jones movie out in the...</td>\n",
       "      <td>Humor, Video</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Interesting Cat Facts</td>\n",
       "      <td>http://catdailynews.com/2023/06/interesting-ca...</td>\n",
       "      <td>June 29, 2023</td>\n",
       "      <td>Many people love cats. However what makes  cat...</td>\n",
       "      <td>Humor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cat Gets “Jailed” For Being Too Daring</td>\n",
       "      <td>http://catdailynews.com/2023/06/cat-gets-jaile...</td>\n",
       "      <td>June 28, 2023</td>\n",
       "      <td>Alice likes traveling in a van on the road wit...</td>\n",
       "      <td>Humor, Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Toys to Keep Your Cat Happy</td>\n",
       "      <td>http://catdailynews.com/2023/06/toys-to-keep-y...</td>\n",
       "      <td>June 27, 2023</td>\n",
       "      <td>Cats need lots of attention, except when they ...</td>\n",
       "      <td>Humor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Evolution of Cats</td>\n",
       "      <td>http://catdailynews.com/2023/06/the-evolution-...</td>\n",
       "      <td>June 26, 2023</td>\n",
       "      <td>People love cats so it’s no surprise that peop...</td>\n",
       "      <td>Science, Egypt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    title  \\\n",
       "0                 Indiana Jones and a Cat   \n",
       "1                   Interesting Cat Facts   \n",
       "2  Cat Gets “Jailed” For Being Too Daring   \n",
       "3             Toys to Keep Your Cat Happy   \n",
       "4                   The Evolution of Cats   \n",
       "\n",
       "                                                link            date  \\\n",
       "0  http://catdailynews.com/2023/06/indiana-jones-...   June 30, 2023   \n",
       "1  http://catdailynews.com/2023/06/interesting-ca...   June 29, 2023   \n",
       "2  http://catdailynews.com/2023/06/cat-gets-jaile...   June 28, 2023   \n",
       "3  http://catdailynews.com/2023/06/toys-to-keep-y...   June 27, 2023   \n",
       "4  http://catdailynews.com/2023/06/the-evolution-...   June 26, 2023   \n",
       "\n",
       "                                                text        category  \n",
       "0  With the latest Indiana Jones movie out in the...    Humor, Video  \n",
       "1  Many people love cats. However what makes  cat...           Humor  \n",
       "2  Alice likes traveling in a van on the road wit...   Humor, Travel  \n",
       "3  Cats need lots of attention, except when they ...           Humor  \n",
       "4  People love cats so it’s no surprise that peop...  Science, Egypt  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('news200.csv', sep='|', names=['title', 'link', 'date', 'text', 'category'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f181e774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of news:  1990\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of news: \", data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c67ce2a",
   "metadata": {},
   "source": [
    "### 1) Preproccessing and cleaning of news data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d62b006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    return \" \".join([w if w not in eng_stopwords else ' ' for w in text.split(' ')])\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return \"\".join([ch if ch not in string.punctuation else ' ' for ch in text])\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return ''.join([i if not i.isdigit() else ' ' for i in text])\n",
    "\n",
    "def remove_multiple_spaces(text):\n",
    "    return re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return \" \".join([lemma.lemmatize(word.lower()) for word in text.split(' ')])\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "eng_stopwords = stopwords.words(\"english\")\n",
    "eng_stopwords.extend(['…', '«', '»', '...'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23fffef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       with latest indiana jones movie theater people...\n",
       "1       many people love cat however make cat even lov...\n",
       "2       alice like traveling van road cat unfortunatel...\n",
       "3       cat need lot attention except want left alone ...\n",
       "4       people love cat it’s surprise people also stud...\n",
       "                              ...                        \n",
       "1985    even hollywood celebrity stay inside prevent s...\n",
       "1986    with people unable watch live sporting event c...\n",
       "1987    if you’ve ever taken picture cat may startled ...\n",
       "1988    erin merryn daughter ginger tabby named carrot...\n",
       "1989    in upton merseyside england employee sainsbury...\n",
       "Name: cleaned_text, Length: 1990, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['cleaned_text'] = data['text'].apply(lambda text: lemmatize_text((remove_multiple_spaces(remove_numbers(remove_stopwords(remove_punctuation(text)))))))\n",
    "data['cleaned_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c588fca",
   "metadata": {},
   "source": [
    "Print unique category sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79a2f1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Humor, Video' 'Humor' 'Humor, Travel' 'Science, Egypt'\n",
      " 'Games, Humor, Technology' 'Humor, England' 'Humor, Technology'\n",
      " 'Homeless cat' 'Humor, Science' 'Games, Technology' 'Games, Humor'\n",
      " 'Science' 'Humor, Show business' 'Humor, Military' 'Humor, Iceland'\n",
      " 'Humor, Japan' 'Technology' 'Humor, Morocco' 'Health' 'Uncategorized'\n",
      " 'Humor, Travel, Thailand' 'Health, Medical' 'Humor, Turkey'\n",
      " 'Homeless cat, Japan' 'Humor, Travel, Poland' 'Homeless cat, Turkey'\n",
      " 'Homeless cat, Humor' 'Humor, India' 'Humor, Thailand'\n",
      " 'Humor, Netherlands, Ukraine' 'Humor, Singapore' 'Health, England'\n",
      " 'Humor, Travel, Japan' 'Humor, Vietnam' 'Homeless cat, Video'\n",
      " 'Humor, Canada' 'Homeless cat, Humor, Show business' 'Humor, Australia'\n",
      " 'Humor, Peru' 'Humor, Ukraine' 'Humor, Sports, Brazil'\n",
      " 'Homeless cat, Humor, Video' 'Humor, Italy' 'Health, Humor'\n",
      " 'Health, Science' 'Medical' 'Health, Homeless cat'\n",
      " 'Humor, Video, England' 'Humor, Greece' 'Crazy cat people, Humor'\n",
      " 'Homeless cat, Ukraine' 'Financial' 'Show business, South Korea'\n",
      " 'Humor, Video, Australia' 'Travel' 'Health, Medical, Science, DNA'\n",
      " 'Financial, Homeless cat, Canada' 'Financial, Humor'\n",
      " 'Humor, Video, Turkey' 'Health, Science, Technology' 'Military, Ukraine'\n",
      " 'Health, Humor, Science' 'Humor, France' 'Humor, China'\n",
      " 'Humor, Technology, Japan' 'Humor, Sports' 'Humor, Science, Poland'\n",
      " 'Humor, Military, Ukraine' 'Humor, Technology, Video'\n",
      " 'Humor, Science, Japan' 'Homeless cat, Humor, Ukraine' 'Humor, Medical'\n",
      " 'Health, Science, DNA' 'Health, Medical, Thailand'\n",
      " 'Humor, Medical, England' 'Humor, Show business, Video'\n",
      " 'Health, Homeless cat, Humor' 'Humor, United Arab Emirates'\n",
      " 'Homeless cat, Technology' 'Science, Germany' 'Video'\n",
      " 'Show business, France' 'Homeless cat, Military, Ukraine' 'Show business'\n",
      " 'Crazy cat people, England' 'Crazy cat people' 'Humor, Video, China'\n",
      " 'Health, Humor, Medical, Science' 'Humor, Travel, France, Ukraine'\n",
      " 'Humor, Military, Russia' 'Health, Medical, Science'\n",
      " 'Humor, South Africa' 'Humor, Sports, France'\n",
      " 'Financial, Humor, Show business' 'Humor, Russia'\n",
      " 'Homeless cat, Humor, England' 'Humor, South Korea' 'Psychic'\n",
      " 'Homeless cat, England' 'Travel, Italy' 'Humor, Malaysia'\n",
      " 'Humor, Science, England' 'Humor, Show business, Japan'\n",
      " 'Health, Humor, Science, Taiwan' 'Humor, Video, New Zealand'\n",
      " 'Health, Humor, England' 'Humor, Show business, Phillippines'\n",
      " 'Science, China' 'Health, Technology' 'Humor, Sports, Technology'\n",
      " 'Humor, Sports, Video' 'Humor, Video, India' 'Humor, Video, Japan'\n",
      " 'Humor, England, Russia' 'Humor, Video, Russia'\n",
      " 'Health, Humor, Medical, Technology' 'Homeless cat, Canada'\n",
      " 'Crazy cat people, Humor, Show business' 'Health, Homeless cat, Medical'\n",
      " 'Health, Canada' 'Humor, Medical, Singapore'\n",
      " 'Health, Humor, Show business' 'Science, Cambodia'\n",
      " 'Homeless cat, Humor, Cyrpus' 'Science, DNA' 'Humor, Nigeria'\n",
      " 'Humor, Show business, Technology' 'Science, India'\n",
      " 'Financial, Technology' 'Humor, Panama' 'Humor, Switzerland'\n",
      " 'Technology, England' 'Health, Australia'\n",
      " 'Health, Humor, Medical, Turkey' 'Homeless cat, Military'\n",
      " 'Financial, Humor, Technology' 'Financial, Health'\n",
      " 'Humor, Travel, England' 'Military, Thailand' 'Health, South Korea'\n",
      " 'Humor, Israel' 'Humor, Show business, Sports'\n",
      " 'Financial, Homeless cat, Humor' 'Health, Humor, Chile'\n",
      " 'Crazy cat people, Canada' 'Humor, Science, China'\n",
      " 'Homeless cat, Philippines' 'Humor, New Zealand'\n",
      " 'Crazy cat people, Humor, Australia' 'Crazy cat people, Financial, Humor'\n",
      " 'Homeless cat, Russia' 'Crazy cat people, Humor, Technology'\n",
      " 'Homeless cat, Humor, Technology' 'Homeless cat, Taiwan'\n",
      " 'Financial, Homeless cat' 'Humor, Show business, Tunisia'\n",
      " 'Crazy cat people, Science, DNA' 'Crazy cat people, Homeless cat, Humor'\n",
      " 'Crazy cat people, Health, Humor, Japan' 'Homeless cat, Science'\n",
      " 'Science, Technology' 'Games, Humor, Technology, Video'\n",
      " 'Crazy cat people, Homeless cat, Humor, Military'\n",
      " 'Crazy cat people, Financial, Homeless cat, Humor'\n",
      " 'Crazy cat people, Show business' 'Humor, Science, Technology'\n",
      " 'Crazy cat people, Humor, England' 'Homeless cat, Travel'\n",
      " 'Crazy cat people, Travel'\n",
      " 'Crazy cat people, Health, Humor, Science, England'\n",
      " 'Homeless cat, Show business' 'Crazy cat people, Humor, Science'\n",
      " 'Crazy cat people, Humor, Science, Peru' 'Crazy cat people, Humor, Japan'\n",
      " 'Humor, Travel, China'\n",
      " 'Crazy cat people, Homeless cat, Humor, Show business'\n",
      " 'Crazy cat people, Homeless cat, Humor, Technology, China'\n",
      " 'Crazy cat people, Homeless cat, Humor, Netherlands'\n",
      " 'Crazy cat people, Humor, Sweden' 'Crazy cat people, Homeless cat'\n",
      " 'Crazy cat people, Humor, Turkey'\n",
      " 'Crazy cat people, Homeless cat, Humor, Travel'\n",
      " 'Crazy cat people, Humor, Science, Japan'\n",
      " 'Crazy cat people, Humor, Video'\n",
      " 'Crazy cat people, Health, Homeless cat, Humor'\n",
      " 'Crazy cat people, Humor, Technology, Australia, China'\n",
      " 'Crazy cat people, Homeless cat, Humor, Italy'\n",
      " 'Crazy cat people, Health, Humor'\n",
      " 'Crazy cat people, Health, Humor, Science'\n",
      " 'Crazy cat people, Humor, Canada' 'Crazy cat people, Humor, Taiwan'\n",
      " 'Crazy cat people, Health' 'Crazy cat people, Humor, Technology, Video'\n",
      " 'Crazy cat people, Humor, Show business, China'\n",
      " 'Crazy cat people, Humor, Show business, England'\n",
      " 'Crazy cat people, Science, England' 'Crazy cat people, Humor, Russia'\n",
      " 'Crazy cat people, Humor, Thailand'\n",
      " 'Crazy cat people, Homeless cat, China'\n",
      " 'Crazy cat people, Health, Medical, Science'\n",
      " 'Crazy cat people, Science, Technology, Egypt'\n",
      " 'Crazy cat people, Humor, New Zealand' 'Crazy cat people, Technology'\n",
      " 'Crazy cat people, China' 'Crazy cat people, Homeless cat, Show business'\n",
      " 'Crazy cat people, Science' 'Crazy cat people, Health, Science'\n",
      " 'Crazy cat people, Humor, Germany'\n",
      " 'Crazy cat people, Homeless cat, Humor, Vietnam' 'Health, Humor, Sports'\n",
      " 'Crazy cat people, Humor, UFO'\n",
      " 'Crazy cat people, Humor, Show business, Japan'\n",
      " 'Crazy cat people, Humor, Show business, Video, England'\n",
      " 'Crazy cat people, Homeless cat, England'\n",
      " 'Crazy cat people, Humor, China' 'Crazy cat people, Humor, Military'\n",
      " 'Crazy cat people, Health, Humor, Video'\n",
      " 'Crazy cat people, Humor, Sports' 'Crazy cat people, Humor, Video, China'\n",
      " 'Crazy cat people, Science, Kazakhstan'\n",
      " 'Crazy cat people, Homeless cat, Humor, New Zealand'\n",
      " 'Crazy cat people, Humor, Travel'\n",
      " 'Crazy cat people, Homeless cat, Medical'\n",
      " 'Crazy cat people, Humor, Video, Thailand'\n",
      " 'Crazy cat people, Humor, Show business, Video'\n",
      " 'Crazy cat people, Humor, South Africa' 'Crazy cat people, Psychic'\n",
      " 'Crazy cat people, Financial' 'Crazy cat people, Games, Humor'\n",
      " 'Crazy cat people, Humor, India'\n",
      " 'Crazy cat people, Financial, Humor, Show business'\n",
      " 'Crazy cat people, Games, Humor, Technology'\n",
      " 'Crazy cat people, Humor, Video, Germany'\n",
      " 'Crazy cat people, Humor, Norway' 'Homeless cat, Science, Australia'\n",
      " 'Crazy cat people, Humor, DNA, Russia'\n",
      " 'Crazy cat people, Humor, Sports, Video, England'\n",
      " 'Crazy cat people, Humor, Show business, Philippines'\n",
      " 'Crazy cat people, Homeless cat, Humor, Thailand'\n",
      " 'Crazy cat people, Games, Humor, Sports'\n",
      " 'Crazy cat people, Health, Medical'\n",
      " 'Crazy cat people, Health, Humor, Japan, Russia'\n",
      " 'Crazy cat people, Humor, Sports, Video'\n",
      " 'Crazy cat people, Health, Homeless cat, Medical'\n",
      " 'Crazy cat people, Humor, Travel, Canada, China'\n",
      " 'Crazy cat people, Homeless cat, Humor, Technology'\n",
      " 'Crazy cat people, Humor, Show business, South Korea'] 247\n"
     ]
    }
   ],
   "source": [
    "print(data['category'].unique(), len(data['category'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3b4522",
   "metadata": {},
   "source": [
    "As you can see articles can have more than one category. \n",
    "\n",
    "Furthermore, country name can also be the last category in category set of article. In this project I will not use counry as a category, that is why I remove all country names from category sets of cat news:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2673df58",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = ['England', 'Vietnam', 'Afghanistan', 'Aland Islands', 'Albania', 'Algeria', 'American Samoa', 'Andorra', 'Angola', 'Anguilla', 'Antarctica', 'Antigua and Barbuda', 'Argentina', 'Armenia', 'Aruba', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium', 'Belize', 'Benin', 'Bermuda', 'Bhutan', 'Bolivia, Plurinational State of', 'Bonaire, Sint Eustatius and Saba', 'Bosnia and Herzegovina', 'Botswana', 'Bouvet Island', 'Brazil', 'British Indian Ocean Territory', 'Brunei Darussalam', 'Bulgaria', 'Burkina Faso', 'Burundi', 'Cambodia', 'Cameroon', 'Canada', 'Cape Verde', 'Cayman Islands', 'Central African Republic', 'Chad', 'Chile', 'China', 'Christmas Island', 'Cocos (Keeling) Islands', 'Colombia', 'Comoros', 'Congo', 'Congo, The Democratic Republic of the', 'Cook Islands', 'Costa Rica', \"Côte d'Ivoire\", 'Croatia', 'Cuba', 'Curaçao', 'Cyprus', 'Czech Republic', 'Denmark', 'Djibouti', 'Dominica', 'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador', 'Equatorial Guinea', 'Eritrea', 'Estonia', 'Ethiopia', 'Falkland Islands (Malvinas)', 'Faroe Islands', 'Fiji', 'Finland', 'France', 'French Guiana', 'French Polynesia', 'French Southern Territories', 'Gabon', 'Gambia', 'Georgia', 'Germany', 'Ghana', 'Gibraltar', 'Greece', 'Greenland', 'Grenada', 'Guadeloupe', 'Guam', 'Guatemala', 'Guernsey', 'Guinea', 'Guinea-Bissau', 'Guyana', 'Haiti', 'Heard Island and McDonald Islands', 'Holy See (Vatican City State)', 'Honduras', 'Hong Kong', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iran, Islamic Republic of', 'Iraq', 'Ireland', 'Isle of Man', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jersey', 'Jordan', 'Kazakhstan', 'Kenya', 'Kiribati', \"Korea, Democratic People's Republic of\", 'Korea, Republic of', 'Kuwait', 'Kyrgyzstan', \"Lao People's Democratic Republic\", 'Latvia', 'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Liechtenstein', 'Lithuania', 'Luxembourg', 'Macao', 'Macedonia, Republic of', 'Madagascar', 'Malawi', 'Malaysia', 'Maldives', 'Mali', 'Malta', 'Marshall Islands', 'Martinique', 'Mauritania', 'Mauritius', 'Mayotte', 'Mexico', 'Micronesia, Federated States of', 'Moldova, Republic of', 'Monaco', 'Mongolia', 'Montenegro', 'Montserrat', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nauru', 'Nepal', 'Netherlands', 'New Caledonia', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'Niue', 'Norfolk Island', 'Northern Mariana Islands', 'Norway', 'Oman', 'Pakistan', 'Palau', 'Palestinian Territory, Occupied', 'Panama', 'Papua New Guinea', 'Paraguay', 'Peru', 'Philippines', 'Pitcairn', 'Poland', 'Portugal', 'Puerto Rico', 'Qatar', 'Réunion', 'Romania', 'Russian Federation', 'Rwanda', 'Saint Barthélemy', 'Saint Helena, Ascension and Tristan da Cunha', 'Saint Kitts and Nevis', 'Saint Lucia', 'Saint Martin (French part)', 'Saint Pierre and Miquelon', 'Saint Vincent and the Grenadines', 'Samoa', 'San Marino', 'Sao Tome and Principe', 'Saudi Arabia', 'Senegal', 'Serbia', 'Seychelles', 'Sierra Leone', 'Singapore', 'Sint Maarten (Dutch part)', 'Slovakia', 'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa', 'South Georgia and the South Sandwich Islands', 'Spain', 'Sri Lanka', 'Sudan', 'Suriname', 'South Sudan', 'Svalbard and Jan Mayen', 'Swaziland', 'Sweden', 'Switzerland', 'Syrian Arab Republic', 'Taiwan, Province of China', 'Tajikistan', 'Tanzania, United Republic of', 'Thailand', 'Timor-Leste', 'Togo', 'Tokelau', 'Tonga', 'Trinidad and Tobago', 'Tunisia', 'Turkey', 'Turkmenistan', 'Turks and Caicos Islands', 'Tuvalu', 'Uganda', 'Ukraine', 'United Arab Emirates', 'United Kingdom', 'United States', 'United States Minor Outlying Islands', 'Uruguay', 'Uzbekistan', 'Vanuatu', 'Venezuela, Bolivarian Republic of', 'Viet Nam', 'Virgin Islands, British', 'Virgin Islands, U.S.', 'Wallis and Futuna', 'Yemen', 'Zambia', 'Zimbabwe']\n",
    "\n",
    "def remove_country_category(category_set):\n",
    "    categories = category_set.split(', ')\n",
    "    categories = [category for category in categories if category not in countries]\n",
    "    categories = ', '.join(categories)\n",
    "    return categories\n",
    "\n",
    "data['categories_without_countries'] = data['category'].apply(lambda category_set: remove_country_category(category_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6958e5",
   "metadata": {},
   "source": [
    "### 2) Formation of topic classification system stages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca19ac6f",
   "metadata": {},
   "source": [
    "Now let's have a look at \"Humor\" category. We can see that this category can be seen in the most of news:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca87df64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1515 out of 1990 news have \"Humor\" category in their category set.\n"
     ]
    }
   ],
   "source": [
    "print(str(len([c_set for c_set in data['categories_without_countries'] if \"Humor\" in c_set])) + ' out of ' + str(len(data)) + ' news have \"Humor\" category in their category set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771dfee4",
   "metadata": {},
   "source": [
    "Number of all possible category sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3685fb3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Humor                                     754\n",
       "Crazy cat people, Humor                   145\n",
       "Health                                    123\n",
       "Humor, Show business                       93\n",
       "Humor, Video                               82\n",
       "Homeless cat                               72\n",
       "Humor, Technology                          62\n",
       "Crazy cat people, Humor, Show business     47\n",
       "Humor, Science                             45\n",
       "Crazy cat people                           39\n",
       "Health, Medical                            36\n",
       "Homeless cat, Humor                        31\n",
       "Science                                    29\n",
       "Health, Humor                              26\n",
       "Humor, Travel                              23\n",
       "Crazy cat people, Health                   19\n",
       "Crazy cat people, Humor, Technology        15\n",
       "Games, Humor, Technology                   15\n",
       "Crazy cat people, Homeless cat             15\n",
       "Crazy cat people, Humor, Video             13\n",
       "Name: categories_without_countries, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['categories_without_countries'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f4c139",
   "metadata": {},
   "source": [
    "From the above cell we can see that the most popular category set consists only from category \"Humor\". After \"Humor\"we can see category sets that contain \"Humor\" plus some other category. \n",
    "\n",
    "I will reduce the amount of categories that i will try to predict. I take for classification task the following categories: \"Humor\", \"Health\", \"Video\", \"Homeless cat\", \"Show business\", \"Technology\", \"Science\". All other categories i add to the new category \"No category\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c4b3daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dataset(category_set):\n",
    "    new_categories = []\n",
    "    categories = category_set.split(', ')\n",
    "    for category in categories:\n",
    "        if category == 'Humor':\n",
    "            new_categories.append(category)\n",
    "            break\n",
    "    for category in categories:\n",
    "        if category in ['Health', 'Video', 'Homeless cat', 'Show business', 'Technology', 'Science']:\n",
    "            new_categories.append(category)\n",
    "            break\n",
    "    if not new_categories:\n",
    "        new_categories.append('No category')\n",
    "    categories = ', '.join(new_categories)\n",
    "    return categories\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f00ce6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52cd2f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['cleaned_category_sets'] = dataset['categories_without_countries'].apply(lambda category_set: reduce_dataset(category_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d0c1dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>categories_without_countries</th>\n",
       "      <th>cleaned_category_sets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Indiana Jones and a Cat</td>\n",
       "      <td>http://catdailynews.com/2023/06/indiana-jones-...</td>\n",
       "      <td>June 30, 2023</td>\n",
       "      <td>With the latest Indiana Jones movie out in the...</td>\n",
       "      <td>Humor, Video</td>\n",
       "      <td>with latest indiana jones movie theater people...</td>\n",
       "      <td>Humor, Video</td>\n",
       "      <td>Humor, Video</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Interesting Cat Facts</td>\n",
       "      <td>http://catdailynews.com/2023/06/interesting-ca...</td>\n",
       "      <td>June 29, 2023</td>\n",
       "      <td>Many people love cats. However what makes  cat...</td>\n",
       "      <td>Humor</td>\n",
       "      <td>many people love cat however make cat even lov...</td>\n",
       "      <td>Humor</td>\n",
       "      <td>Humor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cat Gets “Jailed” For Being Too Daring</td>\n",
       "      <td>http://catdailynews.com/2023/06/cat-gets-jaile...</td>\n",
       "      <td>June 28, 2023</td>\n",
       "      <td>Alice likes traveling in a van on the road wit...</td>\n",
       "      <td>Humor, Travel</td>\n",
       "      <td>alice like traveling van road cat unfortunatel...</td>\n",
       "      <td>Humor, Travel</td>\n",
       "      <td>Humor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Toys to Keep Your Cat Happy</td>\n",
       "      <td>http://catdailynews.com/2023/06/toys-to-keep-y...</td>\n",
       "      <td>June 27, 2023</td>\n",
       "      <td>Cats need lots of attention, except when they ...</td>\n",
       "      <td>Humor</td>\n",
       "      <td>cat need lot attention except want left alone ...</td>\n",
       "      <td>Humor</td>\n",
       "      <td>Humor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Evolution of Cats</td>\n",
       "      <td>http://catdailynews.com/2023/06/the-evolution-...</td>\n",
       "      <td>June 26, 2023</td>\n",
       "      <td>People love cats so it’s no surprise that peop...</td>\n",
       "      <td>Science, Egypt</td>\n",
       "      <td>people love cat it’s surprise people also stud...</td>\n",
       "      <td>Science</td>\n",
       "      <td>Science</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    title  \\\n",
       "0                 Indiana Jones and a Cat   \n",
       "1                   Interesting Cat Facts   \n",
       "2  Cat Gets “Jailed” For Being Too Daring   \n",
       "3             Toys to Keep Your Cat Happy   \n",
       "4                   The Evolution of Cats   \n",
       "\n",
       "                                                link            date  \\\n",
       "0  http://catdailynews.com/2023/06/indiana-jones-...   June 30, 2023   \n",
       "1  http://catdailynews.com/2023/06/interesting-ca...   June 29, 2023   \n",
       "2  http://catdailynews.com/2023/06/cat-gets-jaile...   June 28, 2023   \n",
       "3  http://catdailynews.com/2023/06/toys-to-keep-y...   June 27, 2023   \n",
       "4  http://catdailynews.com/2023/06/the-evolution-...   June 26, 2023   \n",
       "\n",
       "                                                text        category  \\\n",
       "0  With the latest Indiana Jones movie out in the...    Humor, Video   \n",
       "1  Many people love cats. However what makes  cat...           Humor   \n",
       "2  Alice likes traveling in a van on the road wit...   Humor, Travel   \n",
       "3  Cats need lots of attention, except when they ...           Humor   \n",
       "4  People love cats so it’s no surprise that peop...  Science, Egypt   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  with latest indiana jones movie theater people...   \n",
       "1  many people love cat however make cat even lov...   \n",
       "2  alice like traveling van road cat unfortunatel...   \n",
       "3  cat need lot attention except want left alone ...   \n",
       "4  people love cat it’s surprise people also stud...   \n",
       "\n",
       "  categories_without_countries cleaned_category_sets  \n",
       "0                 Humor, Video          Humor, Video  \n",
       "1                        Humor                 Humor  \n",
       "2                Humor, Travel                 Humor  \n",
       "3                        Humor                 Humor  \n",
       "4                      Science               Science  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84181133",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Cat content is funny so it is not suprising that there are so many humorous articles in the dataset. I decided to devide my classification system for two steps:\n",
    "\n",
    "1. Classify news on humorous or not humorous\n",
    "2. Classify news on remaining categories\n",
    "3. Then I will combine results from both classifications and evaluate them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb7b0a1",
   "metadata": {},
   "source": [
    "I will save some data for testing of the final two stages system (humor classification and other categories classification) proportionally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dde5b169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Humor                   994\n",
       "Health                  215\n",
       "Humor, Show business    158\n",
       "Homeless cat            107\n",
       "Humor, Video            101\n",
       "Humor, Technology       101\n",
       "No category              80\n",
       "Humor, Science           57\n",
       "Humor, Homeless cat      54\n",
       "Humor, Health            50\n",
       "Science                  39\n",
       "Technology               19\n",
       "Show business            13\n",
       "Video                     2\n",
       "Name: cleaned_category_sets, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['cleaned_category_sets'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a5147b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_humor = len(dataset[dataset['cleaned_category_sets'] == 'Humor']) - round(len(dataset[dataset['cleaned_category_sets'] == 'Humor']) * 0.25)\n",
    "final_test_dataset_humor = dataset[dataset['cleaned_category_sets'] == 'Humor'][threshold_humor:]\n",
    "working_dataset_humor = dataset[dataset['cleaned_category_sets'] == 'Humor'][:threshold_humor]\n",
    "\n",
    "#threshold_humor_crazy_cat_people = len(dataset[dataset['cleaned_category_sets'] == 'Humor, Crazy cat people']) - round(len(dataset[dataset['cleaned_category_sets'] == 'Humor, Crazy cat people']) * 0.25)\n",
    "#final_test_dataset_humor_crazy_cat_people = dataset[dataset['cleaned_category_sets'] == 'Humor, Crazy cat people'][threshold_humor_crazy_cat_people:]\n",
    "#working_dataset_humor_crazy_cat_people = dataset[dataset['cleaned_category_sets'] == 'Humor, Crazy cat people'][:threshold_humor_crazy_cat_people]\n",
    "\n",
    "threshold_health = len(dataset[dataset['cleaned_category_sets'] == 'Health']) - round(len(dataset[dataset['cleaned_category_sets'] == 'Health']) * 0.25)\n",
    "final_test_dataset_health = dataset[dataset['cleaned_category_sets'] == 'Health'][threshold_health:]\n",
    "working_dataset_health = dataset[dataset['cleaned_category_sets'] == 'Health'][:threshold_health]\n",
    "\n",
    "#threshold_crazy_cat_people = len(dataset[dataset['cleaned_category_sets'] == 'Crazy cat people']) - round(len(dataset[dataset['cleaned_category_sets'] == 'Crazy cat people']) * 0.25)\n",
    "#final_test_dataset_crazy_cat_people = dataset[dataset['cleaned_category_sets'] == 'Crazy cat people'][threshold_crazy_cat_people:]\n",
    "#working_dataset_crazy_cat_people = dataset[dataset['cleaned_category_sets'] == 'Crazy cat people'][:threshold_crazy_cat_people]\n",
    "\n",
    "threshold_humor_show_business = len(dataset[dataset['cleaned_category_sets'] == 'Humor, Show business']) - round(len(dataset[dataset['cleaned_category_sets'] == 'Humor, Show business']) * 0.25)\n",
    "final_test_dataset_humor_show_business = dataset[dataset['cleaned_category_sets'] == 'Humor, Show business'][threshold_humor_show_business:]\n",
    "working_dataset_humor_show_business = dataset[dataset['cleaned_category_sets'] == 'Humor, Show business'][:threshold_humor_show_business]\n",
    "\n",
    "threshold_homeless_cat = len(dataset[dataset['cleaned_category_sets'] == 'Homeless cat']) - round(len(dataset[dataset['cleaned_category_sets'] == 'Homeless cat']) * 0.25)\n",
    "final_test_dataset_homeless_cat = dataset[dataset['cleaned_category_sets'] == 'Homeless cat'][threshold_homeless_cat:]\n",
    "working_dataset_homeless_cat = dataset[dataset['cleaned_category_sets'] == 'Homeless cat'][:threshold_homeless_cat]\n",
    "\n",
    "threshold_humor_video = len(dataset[dataset['cleaned_category_sets'] == 'Humor, Video']) - round(len(dataset[dataset['cleaned_category_sets'] == 'Humor, Video']) * 0.25)\n",
    "final_test_dataset_humor_video = dataset[dataset['cleaned_category_sets'] == 'Humor, Video'][threshold_humor_video:]\n",
    "working_dataset_humor_video = dataset[dataset['cleaned_category_sets'] == 'Humor, Video'][:threshold_humor_video]\n",
    "\n",
    "threshold_humor_technology = len(dataset[dataset['cleaned_category_sets'] == 'Humor, Technology']) - round(len(dataset[dataset['cleaned_category_sets'] == 'Humor, Technology']) * 0.25)\n",
    "final_test_dataset_humor_technology = dataset[dataset['cleaned_category_sets'] == 'Humor, Technology'][threshold_humor_technology:]\n",
    "working_dataset_humor_technology = dataset[dataset['cleaned_category_sets'] == 'Humor, Technology'][:threshold_humor_technology]\n",
    "\n",
    "threshold_humor_science = len(dataset[dataset['cleaned_category_sets'] == 'Humor, Science']) - round(len(dataset[dataset['cleaned_category_sets'] == 'Humor, Science']) * 0.25)\n",
    "final_test_dataset_humor_science = dataset[dataset['cleaned_category_sets'] == 'Humor, Science'][threshold_humor_science:]\n",
    "working_dataset_humor_science = dataset[dataset['cleaned_category_sets'] == 'Humor, Science'][:threshold_humor_science]\n",
    "\n",
    "threshold_humor_homeless_cat = len(dataset[dataset['cleaned_category_sets'] == 'Humor, Homeless cat']) - round(len(dataset[dataset['cleaned_category_sets'] == 'Humor, Homeless cat']) * 0.25)\n",
    "final_test_dataset_humor_homeless_cat = dataset[dataset['cleaned_category_sets'] == 'Humor, Homeless cat'][threshold_humor_homeless_cat:]\n",
    "working_dataset_humor_homeless_cat = dataset[dataset['cleaned_category_sets'] == 'Humor, Homeless cat'][:threshold_humor_homeless_cat]\n",
    "\n",
    "threshold_humor_health = len(dataset[dataset['cleaned_category_sets'] == 'Humor, Health']) - round(len(dataset[dataset['cleaned_category_sets'] == 'Humor, Health']) * 0.25)\n",
    "final_test_dataset_humor_health = dataset[dataset['cleaned_category_sets'] == 'Humor, Health'][threshold_humor_health:]\n",
    "working_dataset_humor_health = dataset[dataset['cleaned_category_sets'] == 'Humor, Health'][:threshold_humor_health]\n",
    "\n",
    "threshold_no_category = len(dataset[dataset['cleaned_category_sets'] == 'No category']) - round(len(dataset[dataset['cleaned_category_sets'] == 'No category']) * 0.25)\n",
    "final_test_dataset_no_category = dataset[dataset['cleaned_category_sets'] == 'No category'][threshold_no_category:]\n",
    "working_dataset_no_category = dataset[dataset['cleaned_category_sets'] == 'No category'][:threshold_no_category]\n",
    "\n",
    "\n",
    "threshold_science = len(dataset[dataset['cleaned_category_sets'] == 'Science']) - round(len(dataset[dataset['cleaned_category_sets'] == 'Science']) * 0.25)\n",
    "final_test_dataset_science = dataset[dataset['cleaned_category_sets'] == 'Science'][threshold_science:]\n",
    "working_dataset_science = dataset[dataset['cleaned_category_sets'] == 'Science'][:threshold_science]\n",
    "\n",
    "threshold_technology = len(dataset[dataset['cleaned_category_sets'] == 'Technology']) - round(len(dataset[dataset['cleaned_category_sets'] == 'Technology']) * 0.25)\n",
    "final_test_dataset_technology = dataset[dataset['cleaned_category_sets'] == 'Technology'][threshold_technology:]\n",
    "working_dataset_technology = dataset[dataset['cleaned_category_sets'] == 'Technology'][:threshold_technology]\n",
    "\n",
    "threshold_show_business = len(dataset[dataset['cleaned_category_sets'] == 'Show business']) - round(len(dataset[dataset['cleaned_category_sets'] == 'Show business']) * 0.25)\n",
    "final_test_dataset_show_business = dataset[dataset['cleaned_category_sets'] == 'Show business'][threshold_show_business:]\n",
    "working_dataset_show_business = dataset[dataset['cleaned_category_sets'] == 'Show business'][:threshold_show_business]\n",
    "\n",
    "threshold_video = len(dataset[dataset['cleaned_category_sets'] == 'Video']) - round(len(dataset[dataset['cleaned_category_sets'] == 'Video']) * 0.25)\n",
    "final_test_dataset_video = dataset[dataset['cleaned_category_sets'] == 'Video'][threshold_video:]\n",
    "working_dataset_video = dataset[dataset['cleaned_category_sets'] == 'Video'][:threshold_video]\n",
    "\n",
    "\n",
    "\n",
    "final_test_frames = [final_test_dataset_humor, final_test_dataset_health,  \n",
    "                   final_test_dataset_show_business, final_test_dataset_homeless_cat, final_test_dataset_humor_video, final_test_dataset_humor_technology, \n",
    "                    final_test_dataset_humor_science, final_test_dataset_humor_homeless_cat, final_test_dataset_humor_health, final_test_dataset_no_category,\n",
    "                    final_test_dataset_science, final_test_dataset_technology, final_test_dataset_show_business, final_test_dataset_video]\n",
    "final_test_dataset = pd.concat(final_test_frames)\n",
    "\n",
    "\n",
    "working_dataset = [working_dataset_humor, working_dataset_health, \n",
    "                   working_dataset_humor_show_business, working_dataset_homeless_cat, working_dataset_humor_video, working_dataset_humor_technology, \n",
    "                    working_dataset_humor_science, working_dataset_humor_homeless_cat, working_dataset_humor_health, working_dataset_no_category,\n",
    "                    working_dataset_science, working_dataset_technology, working_dataset_show_business, working_dataset_video]\n",
    "working_dataset = pd.concat(working_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb18aa7",
   "metadata": {},
   "source": [
    "I will continue working with this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fab95b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Humor                   746\n",
       "Health                  161\n",
       "Humor, Show business    118\n",
       "Homeless cat             80\n",
       "Humor, Video             76\n",
       "Humor, Technology        76\n",
       "No category              60\n",
       "Humor, Science           43\n",
       "Humor, Homeless cat      40\n",
       "Humor, Health            38\n",
       "Science                  29\n",
       "Technology               14\n",
       "Show business            10\n",
       "Video                     2\n",
       "Name: cleaned_category_sets, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_dataset['cleaned_category_sets'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51dd9d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(working_dataset['cleaned_text'], working_dataset['cleaned_category_sets'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaa1e13",
   "metadata": {},
   "source": [
    "### 2) Classification for humorous or not humorous articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227dbe04",
   "metadata": {},
   "source": [
    "### Features engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c05578e",
   "metadata": {},
   "source": [
    "For classification for humorous or not humorous articles I make labels \"Humor\" or \"not Humor\" according each article category set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45bc71bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_categories_exept_humor(category_set):\n",
    "    if \"Humor\" in category_set:\n",
    "        return \"Humor\"\n",
    "    else:\n",
    "        return \"not Humor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8ca0adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dataset['category_humor_task'] = working_dataset['cleaned_category_sets'].apply(lambda category_set: remove_categories_exept_humor(category_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4eee937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of humorous news:  1137\n",
      "Number of not humorous news:  356\n"
     ]
    }
   ],
   "source": [
    "print('Number of humorous news: ', len(working_dataset[working_dataset['category_humor_task'] == \"Humor\"]))\n",
    "print('Number of not humorous news: ', len(working_dataset[working_dataset['category_humor_task'] == \"not Humor\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f9ac07",
   "metadata": {},
   "source": [
    "I want to make number of samples for each class similar for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30f7cf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "humor_train_dataset = working_dataset[working_dataset['category_humor_task'] == \"Humor\"][:500]\n",
    "humor_test_dataset = working_dataset[working_dataset['category_humor_task'] == \"Humor\"][500:]\n",
    "humor_train_dataset = pd.concat([humor_train_dataset, working_dataset[working_dataset['category_humor_task'] == \"not Humor\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9edb57f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(humor_train_dataset['cleaned_text'], humor_train_dataset['category_humor_task'], test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7c6ca6",
   "metadata": {},
   "source": [
    "### Choose the right model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52480930",
   "metadata": {},
   "source": [
    "Let's look at performance of two models: SGDClassifier (linear models with stochastic gradient descent) and KNeighborsClassifier (k-nearest neighbors):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8146ed36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                ('knb_clf', KNeighborsClassifier(n_neighbors=10))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_clf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('sgd_clf', SGDClassifier(random_state=42))])\n",
    "\n",
    "knb_clf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('knb_clf', KNeighborsClassifier(n_neighbors=10))])\n",
    "\n",
    "sgd_clf.fit(X_train, y_train)\n",
    "knb_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d31220",
   "metadata": {},
   "source": [
    "#### SGDClassifier results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c07524a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Humor       0.84      0.74      0.79       110\n",
      "   not Humor       0.62      0.76      0.68        62\n",
      "\n",
      "    accuracy                           0.74       172\n",
      "   macro avg       0.73      0.75      0.73       172\n",
      "weighted avg       0.76      0.74      0.75       172\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_sgd = sgd_clf.predict(X_test)\n",
    "print(metrics.classification_report(predicted_sgd, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c7d6e9",
   "metadata": {},
   "source": [
    "#### KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fddb1089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Humor       0.91      0.70      0.79       124\n",
      "   not Humor       0.51      0.81      0.63        48\n",
      "\n",
      "    accuracy                           0.73       172\n",
      "   macro avg       0.71      0.76      0.71       172\n",
      "weighted avg       0.80      0.73      0.75       172\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_knb = knb_clf.predict(X_test)\n",
    "print(metrics.classification_report(predicted_knb, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87a63bb",
   "metadata": {},
   "source": [
    "SGDClassifier has better results than KNeighborsClassifier, that's why I will use SGDClassifier for the further work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6d82e5",
   "metadata": {},
   "source": [
    "### Hyperparameters optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82092cf9",
   "metadata": {},
   "source": [
    "Next I do hyperparameters optimization for SGDClassifier using 5-fold cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1a80cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                       ('sgd_clf',\n",
       "                                        SGDClassifier(random_state=42))]),\n",
       "             n_jobs=5,\n",
       "             param_grid={'sgd_clf__loss': ['hinge', 'modified_huber'],\n",
       "                         'sgd_clf__penalty': ['l2', 'l1', 'elasticnet']})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_clf = Pipeline(steps=[\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('sgd_clf', SGDClassifier(random_state=42))])\n",
    "\n",
    "parameters = {\n",
    "    'sgd_clf__loss': ['hinge', 'modified_huber'], \n",
    "    'sgd_clf__penalty': ['l2', 'l1', 'elasticnet']\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(sgd_clf, parameters, n_jobs=5)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eec2f16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (CV score=0.773):\n",
      "{'sgd_clf__loss': 'hinge', 'sgd_clf__penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameter (CV score=%0.3f):\" % clf.best_score_)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1b070d",
   "metadata": {},
   "source": [
    "### Humorous articles classification results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a919d87b",
   "metadata": {},
   "source": [
    "I train our SGDClassifier model again with the best hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7924707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Humor       0.81      0.81      0.81        91\n",
      "   not Humor       0.63      0.63      0.63        46\n",
      "\n",
      "    accuracy                           0.75       137\n",
      "   macro avg       0.72      0.72      0.72       137\n",
      "weighted avg       0.75      0.75      0.75       137\n",
      "\n"
     ]
    }
   ],
   "source": [
    "humor_clf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('sgd_clf', SGDClassifier(random_state=42, loss='hinge', penalty='l2'))])\n",
    "\n",
    "\n",
    "humor_clf.fit(X_train, y_train)\n",
    "\n",
    "predicted_humorous_clf = humor_clf.predict(X_val)\n",
    "print(metrics.classification_report(predicted_humorous_clf, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedca0a8",
   "metadata": {},
   "source": [
    "Save model as pickle file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8af7da40",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"humor_clf.pkl\", \"wb\") as file: \n",
    "    pickle.dump(humor_clf, file) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f8081f",
   "metadata": {},
   "source": [
    "The F1 score for predicting humorous news on validation dataset about cats was 0.81, while for predicting non-humorous news about cats it was 0.63. Average F1-score is 0.72."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3d84ee",
   "metadata": {},
   "source": [
    "### 3) Classification for other categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dde89d",
   "metadata": {},
   "source": [
    "### Features engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f968efd",
   "metadata": {},
   "source": [
    "I replace \"Humor\" category from each article set with \"No category\". So now the task is to predict all categories exept \"Humor\" plus \"No category\" category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d2ece3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_humor_category(categories):\n",
    "    categories = categories.split(', ')\n",
    "    if 'Humor' in categories and len(categories) > 1:\n",
    "        categories.remove('Humor')\n",
    "    elif 'Humor' in categories and len(categories) == 1:\n",
    "        categories = [i.replace('Humor', 'No category') for i in categories]\n",
    "    categories = ', '.join(categories)\n",
    "    return categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ae776da",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dataset['category_other_categ_task'] = working_dataset['cleaned_category_sets'].apply(lambda category_set: remove_humor_category(category_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7a91a2",
   "metadata": {},
   "source": [
    "In this dataset we have too many samples that belong to \"No category\" class. First I decided to reduce their amount to make dataset more balanced, but it just made classification results worse. Then I decided to keep the amount of data from different categories as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39a7a295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No category      806\n",
       "Health           199\n",
       "Show business    128\n",
       "Homeless cat     120\n",
       "Technology        90\n",
       "Video             78\n",
       "Science           72\n",
       "Name: category_other_categ_task, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_dataset['category_other_categ_task'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "61a9f7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(working_dataset['cleaned_text'], working_dataset['category_other_categ_task'], test_size=0.2, random_state=23)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd484e7",
   "metadata": {},
   "source": [
    "### Choose the right model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2033e409",
   "metadata": {},
   "source": [
    "For this task let's also look at performance of two models: SGDClassifier (linear models with stochastic gradient descent) and KNeighborsClassifier (k-nearest neighbors):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7009bb78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                ('knb_clf', KNeighborsClassifier(n_neighbors=10))])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_clf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('sgd_clf', SGDClassifier(random_state=42))])\n",
    "\n",
    "knb_clf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('knb_clf', KNeighborsClassifier(n_neighbors=10))])\n",
    "\n",
    "sgd_clf.fit(X_train, y_train)\n",
    "knb_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e4de04",
   "metadata": {},
   "source": [
    "#### SGDClassifier results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f7e865e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Health       0.58      0.70      0.63        37\n",
      " Homeless cat       0.50      0.73      0.59        15\n",
      "  No category       0.87      0.71      0.78       195\n",
      "      Science       0.33      0.50      0.40         8\n",
      "Show business       0.59      0.73      0.65        22\n",
      "   Technology       0.59      0.62      0.61        16\n",
      "        Video       0.12      0.33      0.17         6\n",
      "\n",
      "     accuracy                           0.69       299\n",
      "    macro avg       0.51      0.62      0.55       299\n",
      " weighted avg       0.75      0.69      0.71       299\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_sgd = sgd_clf.predict(X_test)\n",
    "print(metrics.classification_report(predicted_sgd, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2817e113",
   "metadata": {},
   "source": [
    "#### KNeighborsClassifier results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "91a5afec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Health       0.42      0.56      0.48        34\n",
      " Homeless cat       0.50      0.92      0.65        12\n",
      "  No category       0.91      0.64      0.75       225\n",
      "      Science       0.08      1.00      0.15         1\n",
      "Show business       0.37      0.83      0.51        12\n",
      "   Technology       0.41      0.54      0.47        13\n",
      "        Video       0.06      0.50      0.11         2\n",
      "\n",
      "     accuracy                           0.65       299\n",
      "    macro avg       0.39      0.71      0.45       299\n",
      " weighted avg       0.78      0.65      0.69       299\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_knb = knb_clf.predict(X_test)\n",
    "print(metrics.classification_report(predicted_knb, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb65b589",
   "metadata": {},
   "source": [
    "In this task SGDClassifier also worked better then KNeighborsClassifier. so I take SGDClassifier as a model for other categories classification. Firstly I optimize its hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1ebebfa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (CV score=0.672):\n",
      "{'sgd_clf__loss': 'hinge', 'sgd_clf__penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "sgd_clf = Pipeline(steps=[\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('sgd_clf', SGDClassifier(random_state=42))])\n",
    "\n",
    "parameters = {\n",
    "    'sgd_clf__loss': ['hinge', 'modified_huber'], \n",
    "    'sgd_clf__penalty': ['l2', 'l1', 'elasticnet']\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(sgd_clf, parameters, n_jobs=5)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameter (CV score=%0.3f):\" % clf.best_score_)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b644e1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Health       0.38      0.56      0.45        27\n",
      " Homeless cat       0.60      0.52      0.56        23\n",
      "  No category       0.79      0.70      0.74       147\n",
      "      Science       0.27      0.75      0.40         4\n",
      "Show business       0.77      0.67      0.71        15\n",
      "   Technology       0.27      0.33      0.30        12\n",
      "        Video       0.36      0.36      0.36        11\n",
      "\n",
      "     accuracy                           0.63       239\n",
      "    macro avg       0.49      0.56      0.50       239\n",
      " weighted avg       0.67      0.63      0.65       239\n",
      "\n"
     ]
    }
   ],
   "source": [
    "other_categ_clf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('sgd_clf', SGDClassifier(random_state=42, loss='hinge', penalty='l1'))])\n",
    "\n",
    "other_categ_clf.fit(X_train, y_train)\n",
    "\n",
    "predicted_other_categ_clf = other_categ_clf.predict(X_val)\n",
    "print(metrics.classification_report(predicted_other_categ_clf, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f04b5f6",
   "metadata": {},
   "source": [
    "Save model to pickle file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "89e8c444",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"other_categ_clf.pkl\", \"wb\") as file: \n",
    "    pickle.dump(other_categ_clf, file) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa6cdaa",
   "metadata": {},
   "source": [
    "Average F4 score is 0.50 for 7 categories classification task. \"Technology\" and \"Video\" categories  are the worst predicted categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555c41ba",
   "metadata": {},
   "source": [
    "### 4) Сombining two classificators together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727fe7a6",
   "metadata": {},
   "source": [
    "Now I combine two stages - humor classification and other categories classification - in one system and test them together on new data. For testing these two stages together I have data, which i saved earlier. These saved data were not used in training and testing of separate models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdd6d97",
   "metadata": {},
   "source": [
    "### Prediction and results pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e50a581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = final_test_dataset['cleaned_text']\n",
    "\n",
    "predicted_humor_clf = humor_clf.predict(X_test)\n",
    "predicted_other_categ_clf = other_categ_clf.predict(X_test)\n",
    "\n",
    "pred_humor = []\n",
    "for i in predicted_humor_clf:\n",
    "    pred_humor.append(str(i))\n",
    "\n",
    "pred_other_categ = []\n",
    "for i in predicted_other_categ_clf:\n",
    "    pred_other_categ.append(str(i))\n",
    "\n",
    "    \n",
    "combine_results = [a +','+ b for a, b in zip(pred_humor, pred_other_categ)]\n",
    "combine_results = [str_categ.split(',') for str_categ in combine_results]\n",
    "combine_results = pd.Series(combine_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "af21c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_additional_categories(categories):\n",
    "    categ = [i for i in categories if i != 'No category']\n",
    "    categ = [i for i in categ if i != 'not Humor']\n",
    "    return ', '.join(categ)\n",
    "       \n",
    "combine_results_no_category_removed = combine_results.apply(lambda category: remove_additional_categories(category))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892878d1",
   "metadata": {},
   "source": [
    "### Comparison of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7046e34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_full_true_answers = 0\n",
    "final_test_dataset['cleaned_category_sets']\n",
    "for idx_ct, category_true in enumerate(final_test_dataset['cleaned_category_sets']):\n",
    "    for idx_p, category_pred in enumerate(combine_results_no_category_removed):\n",
    "        if idx_ct == idx_p:\n",
    "            if category_true == category_pred:\n",
    "                number_of_full_true_answers += 1\n",
    "\n",
    "number_of_partly_true_answers = 0              \n",
    "final_test_dataset['cleaned_category_sets']\n",
    "for idx_ct, category_true in enumerate(final_test_dataset['cleaned_category_sets']):\n",
    "    for idx_p, category_pred in enumerate(combine_results_no_category_removed):\n",
    "        if idx_ct == idx_p:\n",
    "            category_true_list = category_true.split(',')\n",
    "            category_pred_list = category_pred.split(',')\n",
    "            for categ in category_pred_list:\n",
    "                if categ in category_true_list:\n",
    "                    number_of_partly_true_answers += 1\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9e0d5b",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34037f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242 of 460 category sets were predicted completely right\n",
      "309 of 460 predicted category sets has at least one right predicted category.\n"
     ]
    }
   ],
   "source": [
    "print(str(number_of_full_true_answers) + ' of ' + str(len(final_test_dataset)) + ' category sets were predicted completely right')\n",
    "print(str(number_of_partly_true_answers) + ' of ' + str(len(final_test_dataset)) + ' predicted category sets has at least one right predicted category.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1231021b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy that all categories were predicted correctly is  52.61\n",
      "Accuracy that at least one category was predicted correctly is  67.17\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy that all categories were predicted correctly is ', str(round(number_of_full_true_answers/len(final_test_dataset)*100,2)))\n",
    "print('Accuracy that at least one category was predicted correctly is ', str(round(number_of_partly_true_answers/len(final_test_dataset)*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43211f5f",
   "metadata": {},
   "source": [
    "## 3. Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bc1457",
   "metadata": {},
   "source": [
    "For sentiment analysis I use pre-trained VADER tool. \n",
    "\n",
    "VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media.\n",
    "\n",
    "I don't train algorithm, I just predict if article is positive, negative or neutral with already pre-tarined tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dd3603ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "#calculate the negative, positive, neutral and compound scores, plus verbal evaluation\n",
    "def sentiment_vader(sentence):\n",
    "\n",
    "    # Create a SentimentIntensityAnalyzer object.\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "    negative = sentiment_dict['neg']\n",
    "    neutral = sentiment_dict['neu']\n",
    "    positive = sentiment_dict['pos']\n",
    "    compound = sentiment_dict['compound']\n",
    "\n",
    "    if sentiment_dict['compound'] >= 0.05 :\n",
    "        overall_sentiment = \"Positive\"\n",
    "\n",
    "    elif sentiment_dict['compound'] <= - 0.05 :\n",
    "        overall_sentiment = \"Negative\"\n",
    "\n",
    "    else :\n",
    "        overall_sentiment = \"Neutral\"\n",
    "  \n",
    "    return overall_sentiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dfdb0b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sentiment'] = data['text'].apply(lambda text: sentiment_vader(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "87458917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>categories_without_countries</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Indiana Jones and a Cat</td>\n",
       "      <td>http://catdailynews.com/2023/06/indiana-jones-...</td>\n",
       "      <td>June 30, 2023</td>\n",
       "      <td>With the latest Indiana Jones movie out in the...</td>\n",
       "      <td>Humor, Video</td>\n",
       "      <td>with latest indiana jones movie theater people...</td>\n",
       "      <td>Humor, Video</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Interesting Cat Facts</td>\n",
       "      <td>http://catdailynews.com/2023/06/interesting-ca...</td>\n",
       "      <td>June 29, 2023</td>\n",
       "      <td>Many people love cats. However what makes  cat...</td>\n",
       "      <td>Humor</td>\n",
       "      <td>many people love cat however make cat even lov...</td>\n",
       "      <td>Humor</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cat Gets “Jailed” For Being Too Daring</td>\n",
       "      <td>http://catdailynews.com/2023/06/cat-gets-jaile...</td>\n",
       "      <td>June 28, 2023</td>\n",
       "      <td>Alice likes traveling in a van on the road wit...</td>\n",
       "      <td>Humor, Travel</td>\n",
       "      <td>alice like traveling van road cat unfortunatel...</td>\n",
       "      <td>Humor, Travel</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Toys to Keep Your Cat Happy</td>\n",
       "      <td>http://catdailynews.com/2023/06/toys-to-keep-y...</td>\n",
       "      <td>June 27, 2023</td>\n",
       "      <td>Cats need lots of attention, except when they ...</td>\n",
       "      <td>Humor</td>\n",
       "      <td>cat need lot attention except want left alone ...</td>\n",
       "      <td>Humor</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Evolution of Cats</td>\n",
       "      <td>http://catdailynews.com/2023/06/the-evolution-...</td>\n",
       "      <td>June 26, 2023</td>\n",
       "      <td>People love cats so it’s no surprise that peop...</td>\n",
       "      <td>Science, Egypt</td>\n",
       "      <td>people love cat it’s surprise people also stud...</td>\n",
       "      <td>Science</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    title  \\\n",
       "0                 Indiana Jones and a Cat   \n",
       "1                   Interesting Cat Facts   \n",
       "2  Cat Gets “Jailed” For Being Too Daring   \n",
       "3             Toys to Keep Your Cat Happy   \n",
       "4                   The Evolution of Cats   \n",
       "\n",
       "                                                link            date  \\\n",
       "0  http://catdailynews.com/2023/06/indiana-jones-...   June 30, 2023   \n",
       "1  http://catdailynews.com/2023/06/interesting-ca...   June 29, 2023   \n",
       "2  http://catdailynews.com/2023/06/cat-gets-jaile...   June 28, 2023   \n",
       "3  http://catdailynews.com/2023/06/toys-to-keep-y...   June 27, 2023   \n",
       "4  http://catdailynews.com/2023/06/the-evolution-...   June 26, 2023   \n",
       "\n",
       "                                                text        category  \\\n",
       "0  With the latest Indiana Jones movie out in the...    Humor, Video   \n",
       "1  Many people love cats. However what makes  cat...           Humor   \n",
       "2  Alice likes traveling in a van on the road wit...   Humor, Travel   \n",
       "3  Cats need lots of attention, except when they ...           Humor   \n",
       "4  People love cats so it’s no surprise that peop...  Science, Egypt   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  with latest indiana jones movie theater people...   \n",
       "1  many people love cat however make cat even lov...   \n",
       "2  alice like traveling van road cat unfortunatel...   \n",
       "3  cat need lot attention except want left alone ...   \n",
       "4  people love cat it’s surprise people also stud...   \n",
       "\n",
       "  categories_without_countries sentiment  \n",
       "0                 Humor, Video  Positive  \n",
       "1                        Humor  Positive  \n",
       "2                Humor, Travel  Positive  \n",
       "3                        Humor  Positive  \n",
       "4                      Science  Positive  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c2659e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive cat news out of all news 1600/1990\n",
      "Number of negative cat news out of all news 364/1990\n",
      "Number of neutral cat news out of all news 26/1990\n",
      "Cat news are mostly positive.\n"
     ]
    }
   ],
   "source": [
    "print('Number of positive cat news out of all news', str(len(data[data['sentiment'] == \"Positive\"])) + '/' + str(len(data)))\n",
    "print('Number of negative cat news out of all news', str(len(data[data['sentiment'] == \"Negative\"])) + '/' + str(len(data)))\n",
    "print('Number of neutral cat news out of all news', str(len(data[data['sentiment'] == \"Neutral\"])) + '/' + str(len(data)))\n",
    "print('Cat news are mostly positive.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a31645",
   "metadata": {},
   "source": [
    "## 4. Web-application creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0449ed74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "09e110e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\polin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\polin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import pickle\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "893a81d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter nbextension enable --py widgetsnbextension --sys-prefix\n",
    "# !jupyter serverextension enable voila --sys-prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b2859760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_news_from_main_web_page():\n",
    "    news_data = {'Article title': [], 'Link': [], 'text': [], 'cleaned_text': [], 'Date': [], 'Categories': [], 'Predicted categories': [], 'Sentiment': []}\n",
    "    page = \"http://catdailynews.com/\"\n",
    "    protocol_list = requests.get(page).text\n",
    "    soup = BeautifulSoup(protocol_list, 'lxml')\n",
    "    \n",
    "    for ai in soup.find_all('a'):\n",
    "        if ai.div:\n",
    "            news_data['Article title'].append(ai.div.text)\n",
    "            news_data['Link'].append(ai.get('href'))\n",
    "    return news_data\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([w if w not in eng_stopwords else ' ' for w in text.split(' ')])\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return \"\".join([ch if ch not in string.punctuation else ' ' for ch in text])\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return ''.join([i if not i.isdigit() else ' ' for i in text])\n",
    "\n",
    "def remove_multiple_spaces(text):\n",
    "    return re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return \" \".join([lemma.lemmatize(word.lower()) for word in text.split(' ')])\n",
    "\n",
    "def news_text_cleaning(text):\n",
    "    text_removed_stopwords = remove_stopwords(text)\n",
    "    text_removed_punctuation = remove_punctuation(text_removed_stopwords)\n",
    "    text_removed_numbers = remove_numbers(text_removed_punctuation)\n",
    "    text_removed_multiple_spaces = remove_multiple_spaces(text_removed_numbers)\n",
    "    text_cleaned = lemmatize_text(text_removed_multiple_spaces)\n",
    "    return text_cleaned\n",
    "\n",
    "def predict_humor(text):\n",
    "    return humor_clf.predict([text])\n",
    "\n",
    "def predict_other_categories(text):\n",
    "    return other_categ_clf.predict([text])\n",
    "\n",
    "def predict_categories(text):\n",
    "    predicted_humor = predict_humor(text)\n",
    "    predicted_humor = str(predicted_humor[0])\n",
    "    predicted_other_categ = predict_other_categories(text)\n",
    "    predicted_other_categ = str(predicted_other_categ[0])\n",
    "    if predicted_humor == 'not Humor':\n",
    "        return predicted_other_categ\n",
    "    elif predicted_humor == 'Humor' and predicted_other_categ == 'No category':\n",
    "        return predicted_humor\n",
    "    else:\n",
    "        return predicted_humor + ', ' + predicted_other_categ\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "eng_stopwords = stopwords.words(\"english\")\n",
    "eng_stopwords.extend(['…', '«', '»', '...'])\n",
    "\n",
    "with open(\"humor_clf.pkl\", \"rb\") as file:\n",
    "    humor_clf = pickle.load(file)\n",
    "    \n",
    "with open(\"other_categ_clf.pkl\", \"rb\") as file:\n",
    "    other_categ_clf = pickle.load(file)\n",
    "    \n",
    "def sentiment_vader(sentence):\n",
    "\n",
    "    # Create a SentimentIntensityAnalyzer object.\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "    negative = sentiment_dict['neg']\n",
    "    neutral = sentiment_dict['neu']\n",
    "    positive = sentiment_dict['pos']\n",
    "    compound = sentiment_dict['compound']\n",
    "\n",
    "    if sentiment_dict['compound'] >= 0.05 :\n",
    "        overall_sentiment = \"Positive\"\n",
    "\n",
    "    elif sentiment_dict['compound'] <= - 0.05 :\n",
    "        overall_sentiment = \"Negative\"\n",
    "\n",
    "    else :\n",
    "        overall_sentiment = \"Neutral\"\n",
    "  \n",
    "    return overall_sentiment\n",
    "\n",
    "def predict_category(news_data):\n",
    "    for idx, link in enumerate(news_data['Link']):\n",
    "        protocol_list = requests.get(link).text\n",
    "        soup = BeautifulSoup(protocol_list, 'lxml')\n",
    "\n",
    "        text_parts = soup.find_all('p')\n",
    "        text = ''\n",
    "        for p in text_parts:\n",
    "            text += p.text\n",
    "        news_data['text'].append(text)\n",
    "        text_cleaned = news_text_cleaning(text)\n",
    "        news_data['cleaned_text'].append(text_cleaned)\n",
    "\n",
    "        for divi in soup.find_all('div'):\n",
    "            if divi.get(\"class\") == ['pull-left']:\n",
    "                date = divi.text.split('  ')[0][:-2]\n",
    "        news_data['Date'].append(date)\n",
    "\n",
    "        categories = []\n",
    "        for divi in soup.find_all('div'):\n",
    "            if divi.get(\"class\") == ['category-tag']:\n",
    "                all_a = divi.find_all('a')\n",
    "                for a in all_a:\n",
    "                    if a.text[0].isupper():\n",
    "                        categories.append(a.text)\n",
    "                categories = ', '.join(categories)\n",
    "        news_data['Categories'].append(categories)\n",
    "\n",
    "        pred_categories = predict_categories(text_cleaned)\n",
    "        news_data['Predicted categories'].append(pred_categories)\n",
    "        \n",
    "        pred_sentiment = sentiment_vader(text_cleaned)\n",
    "        news_data['Sentiment'].append(pred_sentiment)\n",
    "    return news_data\n",
    "\n",
    "\n",
    "def make_clickable(val):\n",
    "    # target _blank to open new window\n",
    "    return '<a target=\"_blank\" href=\"{}\">{}</a>'.format(val, val)\n",
    "\n",
    "def create_news_table_show(news_data):\n",
    "    news_data_table = pd.DataFrame(news_data)\n",
    "    news_table_show = news_data_table[['Date', 'Article title', 'Link', 'Categories', 'Predicted categories', 'Sentiment']]\n",
    "    blankIndex=[''] * len(news_table_show)\n",
    "    news_table_show.index=blankIndex\n",
    "    news_table_show = news_table_show.style.format({'Link': make_clickable})\n",
    "    return news_table_show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cba940db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Widget\n",
    "\n",
    "file = open(\"cat_daily_news_main.jpg\", \"rb\")\n",
    "main_image = file.read()\n",
    "\n",
    "file = open(\"sleepy_kitten.jpg\", \"rb\")\n",
    "lovely_image = file.read()\n",
    "\n",
    "image_headline = widgets.Image(\n",
    "                    value=main_image,\n",
    "                    format='jpg',\n",
    "                    width='95%',\n",
    "                    height='500px'\n",
    "                )\n",
    "\n",
    "image_base = widgets.Image(\n",
    "                    value=lovely_image,\n",
    "                    format='jpg',\n",
    "                    width='350px',\n",
    "                )\n",
    "\n",
    "label_base = widgets.Label(\n",
    "                    value='Photo by fanibani',\n",
    "                    style={'description_width': 'initial'}\n",
    "                )\n",
    "\n",
    "\n",
    "vbox_headline_image = widgets.VBox([image_headline])\n",
    "vbox_base_image = widgets.VBox([image_base, label_base])\n",
    "# display(vbox_headline_image, vbox_base_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b7cdf1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# button to load news\n",
    "\n",
    "button_send = widgets.Button(\n",
    "                description='Load news',\n",
    "                tooltip='Send',\n",
    "                style={'description_width': 'initial'}\n",
    "            )\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_button_clicked(event):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        news_data = read_news_from_main_web_page()\n",
    "        news_data_predicted = predict_category(news_data)\n",
    "        news_table_show = create_news_table_show(news_data_predicted)\n",
    "        display(news_table_show)\n",
    "        \n",
    "button_send.on_click(on_button_clicked)\n",
    "\n",
    "vbox_result = widgets.VBox([button_send, output])\n",
    "# display(vbox_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "002ea544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# news table widget\n",
    "\n",
    "# def news_table_widget_creation(df):\n",
    "    \n",
    "#     out = widgets.Output(layout={'border': '1px solid black'})\n",
    "#     with out:\n",
    "#         display(df)\n",
    "#     return out\n",
    "\n",
    "# news_table = news_table_widget_creation(news_table_show)\n",
    "# vbox_news_table = widgets.VBox([news_table])\n",
    "# display(vbox_news_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0c2cbd06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b979538625e546a1870f2471d91d0dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00\\xf0\\x00\\xf0\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f9f0aecbfd649f0a1bbc2bce6995bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe1\"\\x98Exif\\x00\\x00II*\\x00\\x08\\x00\\x00\\x00\\x13\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "level1 = widgets.HBox([vbox_headline_image])\n",
    "level2 = widgets.HBox([vbox_base_image, vbox_result])\n",
    "# level2 = widgets.HBox([vbox_result])\n",
    "# level2 = widgets.HBox([vbox_images, vbox_result])\n",
    "display(level1,level2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc03589c",
   "metadata": {},
   "source": [
    "Then I appload Web-application code to GitHub and make an application using Mybinder.org. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565715fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
